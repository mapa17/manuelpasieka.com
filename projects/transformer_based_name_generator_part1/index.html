<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=theme-color content="#494f5c"><meta name=msapplication-TileColor content="#494f5c"><meta itemprop=name content="Transformer based Name Generator - Part 1"><meta itemprop=description content="This is the first of a three part blog post series in which we are going to develop a Transformer based name generator
Exploring the model and generating the first names Evaluate the quality of the generated names Perform hyper parameter tuning to better understand the model architecture and improve data quality Overview and Introduction In this blog post series you will learn how to build a pytorch model that learns to generate new names from a list of examples."><meta itemprop=datePublished content="2022-04-26T00:00:00+00:00"><meta itemprop=dateModified content="2022-04-26T00:00:00+00:00"><meta itemprop=wordCount content="2272"><meta itemprop=keywords content><meta property="og:title" content="Transformer based Name Generator - Part 1"><meta property="og:description" content="This is the first of a three part blog post series in which we are going to develop a Transformer based name generator
Exploring the model and generating the first names Evaluate the quality of the generated names Perform hyper parameter tuning to better understand the model architecture and improve data quality Overview and Introduction In this blog post series you will learn how to build a pytorch model that learns to generate new names from a list of examples."><meta property="og:type" content="article"><meta property="og:url" content="https://www.manuelpasieka.com/projects/transformer_based_name_generator_part1/"><meta property="article:section" content="projects"><meta property="article:published_time" content="2022-04-26T00:00:00+00:00"><meta property="article:modified_time" content="2022-04-26T00:00:00+00:00"><meta property="og:site_name" content="Manuel Pasieka"><meta name=twitter:card content="summary"><meta name=twitter:title content="Transformer based Name Generator - Part 1"><meta name=twitter:description content="This is the first of a three part blog post series in which we are going to develop a Transformer based name generator
Exploring the model and generating the first names Evaluate the quality of the generated names Perform hyper parameter tuning to better understand the model architecture and improve data quality Overview and Introduction In this blog post series you will learn how to build a pytorch model that learns to generate new names from a list of examples."><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color><link rel="shortcut icon" href=/favicon.ico><title>Transformer based Name Generator - Part 1</title><link rel=stylesheet href=https://www.manuelpasieka.com/css/style.min.60545940dc68bc77005f62ee292cbffcba0be595532826a26a245b63372af945.css integrity="sha256-YFRZQNxovHcAX2LuKSy//LoL5ZVTKCaiaiRbYzcq+UU=" crossorigin=anonymous></head><body id=page><header id=site-header class="animated slideInUp"><div class="hdr-wrapper section-inner"><div class=hdr-left><div class=site-branding><img src=/img/Logo_cutout.png height=28 style=position:relative;top:7px>
<a href=https://www.manuelpasieka.com>Manuel Pasieka</a></div><nav class="site-nav hide-in-mobile"><a href=https://www.manuelpasieka.com/services/>Dienstleistungen</a>
<a href=https://www.manuelpasieka.com/projects/>Projekte</a>
<a href=https://www.manuelpasieka.com/aaip/>Austrian Ai Podcast</a>
<a href=https://www.manuelpasieka.com/about-me/>Über mich</a></nav></div><div class="hdr-right hdr-icons"><span class="hdr-social hide-in-mobile"><a href=https://linkedin.com/in/manuelpasieka target=_blank rel="noopener me" title=Linkedin><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a><a href=https://github.com/mapa17 target=_blank rel="noopener me" title=Github><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></span><button id=menu-btn class=hdr-btn title=Menü><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button></div></div></header><div id=mobile-menu class="animated fast"><ul><li><a href=https://www.manuelpasieka.com/services/>Dienstleistungen</a></li><li><a href=https://www.manuelpasieka.com/projects/>Projekte</a></li><li><a href=https://www.manuelpasieka.com/aaip/>Austrian Ai Podcast</a></li><li><a href=https://www.manuelpasieka.com/about-me/>Über mich</a></li></ul></div><main class="site-main section-inner thin animated fadeIn faster"><h1>Transformer based Name Generator - Part 1</h1><div class=content><p>This is the first of a three part blog post series in which we are going to develop a
Transformer based name generator</p><ul><li><ol><li>Exploring the model and generating the first names</li></ol></li><li><ol start=2><li>Evaluate the quality of the generated names</li></ol></li><li><ol start=3><li>Perform hyper parameter tuning to better understand the model architecture and improve data quality</li></ol></li></ul><h2 id=overview-and-introduction>Overview and Introduction<a href=#overview-and-introduction class=anchor aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 015 5 5 5 0 01-5 5h-3m-6 0H6a5 5 0 01-5-5 5 5 0 015-5h3"/><line x1="8" y1="12" x2="16" y2="12"/></svg></a></h2><p>In this blog post series you will learn how to build a pytorch model that learns
to generate new names from a list of examples.</p><p>The machine learning model will be based on the very successful and widely applied
transformer architecture.
For a excellent introduction to transformers and self attention as their key
idea have a look a <a href=http://peterbloem.nl/blog/transformers>this blog post by Peter Bloem</a>.</p><p>Once we have trained our model and used it to generate a set of names, we are going
to focus on evaluating the generated names, discussing what we want to achieve
with a generative model and how to quantify the data quality.</p><p>Once we have some good data quality metrics, we shift our focus on hyperparameter
tuning with <a href=https://docs.wandb.ai/quickstart>wandb</a> to improve our results
and in addition get a better understanding of our model.</p><p>I hope you are looking forward to this series and find it useful and inspiring.</p><h2 id=project-repository-and-system-configuration>Project Repository and System configuration<a href=#project-repository-and-system-configuration class=anchor aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 015 5 5 5 0 01-5 5h-3m-6 0H6a5 5 0 01-5-5 5 5 0 015-5h3"/><line x1="8" y1="12" x2="16" y2="12"/></svg></a></h2><p>All the code for this tutorial series is available on github as the
<a href=https://github.com/mapa17/onomatico>onomatico</a> project.</p><p>The project makes use of <a href=https://python-poetry.org/docs/basic-usage/>poetry</a>
as the dependency management system, and I as always, recommend to you, to configure
your python projects in a virtual environment based on conda + pip.</p><p>If you dont have a local working environment, or you want to make use of the power
of the cloud, I recommend to check out an previous blog post of mine that helps you to
<a href=ML_dev_deployment_on_AWS.md>setup your ML development environment in the cloud in less than 5 min</a>.</p><h2 id=lets-start-at-the-beginning-the-training-data>Lets start at the beginning, the training data<a href=#lets-start-at-the-beginning-the-training-data class=anchor aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 015 5 5 5 0 01-5 5h-3m-6 0H6a5 5 0 01-5-5 5 5 0 015-5h3"/><line x1="8" y1="12" x2="16" y2="12"/></svg></a></h2><p>The goal of this tutorial is to build a generative model that is capable of creating
american name pairs of first and last names. As all machine learning models we
need training data to teach the model based on positive examples what &ldquo;good&rdquo; names
look like.</p><p>The training data we are going to use is build based on a set of the 100 most
common american first and last names available <a href=https://github.com/fivethirtyeight/data/tree/master/most-common-name>here</a>.</p><p>To be more precise, it contains the 100 most common last names, and combines
them with 10 random picked first names from the 100 most common first names.</p><p>The result is a data set of 1000 name combinations from which we will take 900
to train our generative model and 100 to guide the training process.</p><h2 id=project-structure-and-components>Project Structure and Components<a href=#project-structure-and-components class=anchor aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 015 5 5 5 0 01-5 5h-3m-6 0H6a5 5 0 01-5-5 5 5 0 015-5h3"/><line x1="8" y1="12" x2="16" y2="12"/></svg></a></h2><p>Once you cloned the project repo <a href=https://github.com/mapa17/onomatico>onomatico</a> (for the curious, the name was inspired by a wordplay on <a href=https://en.wikipedia.org/wiki/Onomastics>onomatics</a>, the study of names.) you can find the following
project structure and sub-folders containing</p><ul><li><strong>main folder</strong>: several configuration files to setup the project and configure additional tools (poetry, wandb)</li><li><strong>data folder</strong>: the training and test data used in this blog post</li><li><strong>onomatico</strong>: the python modules tha are used to train a model and generate new names</li><li><strong>deployment</strong>: Terraform configuration and system configuration files to launch AWS instances (fore more details <a href=ML_dev_deployment_on_AWS.md>see</a></li></ul><p>For the rest of this tutorial we will focus on the <code>onomatico</code> folder that contains <code>onomatico/main.py</code>
which provides the frontend in the form of an CLI application and <code>onomatico/utils</code>
that provides two python modules to help access and abstract the training data <code>onomatico/utils/Names.py</code>
and defines our model <code>onomatico/utils/Transformer.py</code>.</p><p>We start with the last two utility modules, explaining first the Transformer model and than how to access the training data.</p><h2 id=a-transformer-based-generative-character-model>A Transformer based generative character model<a href=#a-transformer-based-generative-character-model class=anchor aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 015 5 5 5 0 01-5 5h-3m-6 0H6a5 5 0 01-5-5 5 5 0 015-5h3"/><line x1="8" y1="12" x2="16" y2="12"/></svg></a></h2><p>The code used to build this model is derived from the official <a href=https://pytorch.org/tutorials/beginner/transformer_tutorial.html>Transformer Tutorial</a>
and contains the main Transformer class that is building a Transformer model
on sequences of individual characters.</p><p>The class constructor, combines multiple <a href=https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html>TransformerEncoderLayers</a>
(which make up the heart of the Transformer) preceded by an Embedding layer
that learns distributed representations of our tokens and a positional encoding function
that modulates the embedded tokens to retain information about their position
in the input sequence. At the output of the Transformer we have a linear layer
that maps to our set of tokens, which in our case are individual characters.</p><p>An overview of the Transformer architecture you can see in the following diagram
(taken form <a href=https://charon.me/posts/pytorch/pytorch_seq2seq_6/>this blog post</a>)
<img src=images/TransformerArchitecture.jpg alt="Transformer Architecture"></p><p>Lets have a look at <code>onomatico/utils/Transformer.py</code> in a bit more detail</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>inspect</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>Dict</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span><span class=p>,</span> <span class=n>Tensor</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.nn</span> <span class=kn>import</span> <span class=n>TransformerEncoder</span><span class=p>,</span> <span class=n>TransformerEncoderLayer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>generate_square_subsequent_mask</span><span class=p>(</span><span class=n>sz</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Generates an upper-triangular matrix of -inf, with zeros on diag.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>triu</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>sz</span><span class=p>,</span> <span class=n>sz</span><span class=p>)</span> <span class=o>*</span> <span class=nb>float</span><span class=p>(</span><span class=s2>&#34;-inf&#34;</span><span class=p>),</span> <span class=n>diagonal</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>PositionalEncoding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Add absolute position encodings to embedding vectors.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    For each embedding token add an (additative) position dependent term that
</span></span></span><span class=line><span class=cl><span class=s2>    is calculated as an superposition of sinus and cosines functions that is
</span></span></span><span class=line><span class=cl><span class=s2>    unique for each position in a sequence.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    For a simple visual instruction watch: https://youtu.be/dichIcUZfOw?t=318
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Copied from
</span></span></span><span class=line><span class=cl><span class=s2>    https://pytorch.org/tutorials/beginner/transformer_tutorial.html#load-and-batch-data
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>,</span> <span class=n>max_len</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>5000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>position</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>max_len</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>div_term</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=o>-</span><span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mf>10000.0</span><span class=p>)</span> <span class=o>/</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>max_len</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s2>&#34;pe&#34;</span><span class=p>,</span> <span class=n>pe</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            x: Tensor, shape [seq_len, batch_size, embedding_dim]
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>pe</span><span class=p>[:</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TransformerModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>kwargs</span><span class=p>:</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=kc>None</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>ntokens</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>d_model</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>nhead</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>d_hid</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>nlayers</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Transformer based generative model learning to reproduce token sequences.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            ntokens (int): Size of the vocab used for training and generation
</span></span></span><span class=line><span class=cl><span class=s2>            d_model (int): Embedding size of the input and output of the Feed Forward block of a single Transformer Layer
</span></span></span><span class=line><span class=cl><span class=s2>            nhead (int): number of self attention heads
</span></span></span><span class=line><span class=cl><span class=s2>            d_hid (int): Internal Embedding size of the Feed Forward block (emb sizes: d_model x d_hid x d_model)
</span></span></span><span class=line><span class=cl><span class=s2>            nlayers (int): Number of Transformer Layers
</span></span></span><span class=line><span class=cl><span class=s2>            dropout (float, optional): Dropout rate used for the Multi-headed attention and Feed Forward block. Defaults to 0.5.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Store the kwargs as a dict, so they can be saved with the model</span>
</span></span><span class=line><span class=cl>        <span class=c1># and reused when loading the model.</span>
</span></span><span class=line><span class=cl>        <span class=n>s</span> <span class=o>=</span> <span class=n>inspect</span><span class=o>.</span><span class=n>signature</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=fm>__init__</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>l</span> <span class=o>=</span> <span class=nb>locals</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>kwargs</span> <span class=o>=</span> <span class=p>{</span><span class=n>k</span><span class=p>:</span> <span class=n>l</span><span class=p>[</span><span class=n>k</span><span class=p>]</span> <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>s</span><span class=o>.</span><span class=n>parameters</span><span class=o>.</span><span class=n>keys</span><span class=p>()}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model_type</span> <span class=o>=</span> <span class=s2>&#34;Transformer&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoder</span> <span class=o>=</span> <span class=n>PositionalEncoding</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder_layers</span> <span class=o>=</span> <span class=n>TransformerEncoderLayer</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>nhead</span><span class=p>,</span> <span class=n>d_hid</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>transformer_encoder</span> <span class=o>=</span> <span class=n>TransformerEncoder</span><span class=p>(</span><span class=n>encoder_layers</span><span class=p>,</span> <span class=n>nlayers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>ntokens</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_model</span> <span class=o>=</span> <span class=n>d_model</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>ntokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>init_weights</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>init_weights</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>initrange</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>uniform_</span><span class=p>(</span><span class=o>-</span><span class=n>initrange</span><span class=p>,</span> <span class=n>initrange</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=o>.</span><span class=n>bias</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>uniform_</span><span class=p>(</span><span class=o>-</span><span class=n>initrange</span><span class=p>,</span> <span class=n>initrange</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Use the defined Model components and perform the actual computation.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        As input we expect an tensor that contains multiple input sequences (all of 
</span></span></span><span class=line><span class=cl><span class=s2>        the same length) and a mask tensor that indicates for each position in
</span></span></span><span class=line><span class=cl><span class=s2>        the sequence what other positions can be used  in the self attention mechanism. 
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        The output are raw, unnormalized scores (logits) for each token position
</span></span></span><span class=line><span class=cl><span class=s2>        that have to be fed to an activation function (e.g softmax) in order to
</span></span></span><span class=line><span class=cl><span class=s2>        be interpreted as a probability distribution of the vocabulary.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            src: Tensor, shape [seq_len, batch_size]
</span></span></span><span class=line><span class=cl><span class=s2>            src_mask: Tensor, shape [seq_len, seq_len]
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            output Tensor of shape [seq_len, batch_size, ntoken]
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>src</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=n>src</span><span class=p>)</span> <span class=o>*</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>src</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoder</span><span class=p>(</span><span class=n>src</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>transformer_encoder</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span></code></pre></div><p>For a discussion about the architecture of Transformers I can recommend
<a href=https://e2eml.school/transformers.html>this</a>
and <a href=https://towardsdatascience.com/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3>the following</a> reference.</p><p>With the hyperparameter that are provided during the constructor of the class we
can change the architecture of our model by for example increasing the size of
the internal embedding layers used in <code>TransformerEncoderLayers</code>, the number of
heads or the number of layers themselves. In addition we are specifying the
number of tokens in our vocabulary for the model to be able to read all inputs
and generate names containing only characters that are present in our vocabulary.</p><p>Finding the best hyper parameters for our model is a challenging task, and we will
discuss it later in detail, but for now, as a quick peek about what is to come,
we will see that increasing the embedding sizes has little positive effect on
data quality, but increasing the number heads does.</p><h2 id=loading-and-preprocessing-of-the-dataset>Loading and Preprocessing of the dataset<a href=#loading-and-preprocessing-of-the-dataset class=anchor aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 015 5 5 5 0 01-5 5h-3m-6 0H6a5 5 0 01-5-5 5 5 0 015-5h3"/><line x1="8" y1="12" x2="16" y2="12"/></svg></a></h2><p>The dataset is a csv file containing in a single column the first and last name.
Whereby the last name is spelled on purpose in all capital letters.</p><p>Example</p><pre tabindex=0><code class=language-csv data-lang=csv>name
Jeffrey SMITH
Amanda SMITH
Justin SMITH
Michelle SMITH
Jennifer SMITH
Kathleen SMITH
Linda SMITH
Larry SMITH
Jacob SMITH
...
</code></pre><p>The idea behind the special writing of the Last Name in only capital letters, is
to include certain amount of structural information that our generative
model needs to learn to reproduce, in addition to the individual character distribution.</p><p>The module <code>onomatico/utils/Names.py</code> contains two classes that help with loading
the data and creating a pytorch <a href=https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset>Dataset</a> (i.e <code>NamesDataset</code>)
and another class (i.e. <code>Names</code>) that we use to iterate over the dataset
during training to provide us with pairs of training data (i.e x, y) in mini-batches.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Names</span><span class=p>(</span><span class=n>Iterator</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Wraps a `NamesDataset` providing an iterator that is used during training, returning
</span></span></span><span class=line><span class=cl><span class=s2>    mini batches of data points and raising StopIteration after one iteration over
</span></span></span><span class=line><span class=cl><span class=s2>    the complete dataset (i.e. epoch).
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>names_dataset</span><span class=p>:</span> <span class=n>NamesDataset</span>
</span></span><span class=line><span class=cl>    <span class=n>names_dl</span><span class=p>:</span> <span class=n>DataLoader</span>
</span></span><span class=line><span class=cl>    <span class=n>names_iter</span><span class=p>:</span> <span class=n>Iterator</span>
</span></span><span class=line><span class=cl>    <span class=n>device</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span><span class=p>:</span> <span class=nb>int</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>csv_file</span><span class=p>:</span> <span class=n>Path</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>device</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>,</span> <span class=o>**</span><span class=n>param</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Build a torch DataLoader reading from given csv_file
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            csv_file (Path): Path to csv file containing names
</span></span></span><span class=line><span class=cl><span class=s2>            batch_size (int): Batch size
</span></span></span><span class=line><span class=cl><span class=s2>            device (torch.device): torch device
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>names_dataset</span> <span class=o>=</span> <span class=n>NamesDataset</span><span class=p>(</span><span class=n>csv_file</span><span class=p>,</span> <span class=o>**</span><span class=n>param</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>names_dl</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>names_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>names_iter</span> <span class=o>=</span> <span class=nb>iter</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>names_dl</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>batch_size</span> <span class=o>=</span> <span class=n>batch_size</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_padded_sequence_length</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>names_dataset</span><span class=o>.</span><span class=n>padded_sequence_length</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_batch</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Returns a mini-batch of names, with the target is shifted by one position.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            Tuple[torch.tensor, torch.tensor]: Mini-batch of (training, target)
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># Get a mini-batch of encoded name token sequences</span>
</span></span><span class=line><span class=cl>        <span class=n>batch</span> <span class=o>=</span> <span class=nb>next</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>names_iter</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># The target is shifted by one element</span>
</span></span><span class=line><span class=cl>        <span class=c1># data: &#34;&lt;Bob MILLER&gt;!!&#34;,</span>
</span></span><span class=line><span class=cl>        <span class=c1># target: &#34;Bob MILLER&gt;!!!&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>data</span> <span class=o>=</span> <span class=n>batch</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>target</span> <span class=o>=</span> <span class=n>batch</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>data</span><span class=p>,</span> <span class=n>target</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__len__</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>names_dl</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__iter__</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Iterator</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__next__</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Wrap the iterator generated from the DataLoader.
</span></span></span><span class=line><span class=cl><span class=s2>        Make sure to get refresh the iterator once it is emptied.
</span></span></span><span class=line><span class=cl><span class=s2>        So for a single epoch we raise a StopIteration exception, but can read
</span></span></span><span class=line><span class=cl><span class=s2>        from the Dataset again in the next epoch.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Raises:
</span></span></span><span class=line><span class=cl><span class=s2>            e: StopIteration
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            Tuple[torch.tensor, torch.tensor]: Training and Label data 
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_batch</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>except</span> <span class=ne>StopIteration</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>names_iter</span> <span class=o>=</span> <span class=nb>iter</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>names_dl</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=n>e</span>
</span></span></code></pre></div><p>Because of the small size of the training data, we load the complete data set into
memory and transform it before training (see <code>__create_vocab_and_tokens()</code>).
Creating a list of token sequences of the same length, surrounding each name with
a <code>start</code> (<code>&lt;</code>), an <code>end</code> (<code>></code>) and a padding (<code>!</code>) token.</p><p>Example: <code>Bob MILLER -> &lt;Bob MILLER>!!!</code></p><p>The start token will be used during the generation of new names as the <code>seed</code>
symbol for our model and based on the end token we can identify when the model
&rsquo;thinks&rsquo; that it has completed it job and finished generating a name.</p><p>The padding token is used to have the same sequence length for each name, so we
can batch multiple names together in a tensor of the same size.</p><h2 id=cli-interface>CLI Interface<a href=#cli-interface class=anchor aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 015 5 5 5 0 01-5 5h-3m-6 0H6a5 5 0 01-5-5 5 5 0 015-5h3"/><line x1="8" y1="12" x2="16" y2="12"/></svg></a></h2><p>Part of the project is a CLI interface (<code>onomatico/main.py</code>) that has the following sub commands that can be used to:</p><ul><li><code>vocab</code>: Create a vocab out of CSV training data, that is needed for model training and generation of new names.</li><li><code>train</code>: Train a model using a vocab and training data.</li><li><code>generate</code>: Generate new names using the trained model and a vocab.</li><li><code>compare</code>: Compare the similarity between original and generated names.</li></ul><p>You can explore the arguments required for each command with</p><p><code>onomatico --help</code></p><p>The CLI is build making use of the <a href=https://typer.tiangolo.com/>Typer</a> library that is a derivation of <a href=https://click.palletsprojects.com/en/8.1.x/>Click</a> to reduce the boiler plait code make use of the docstrings and type hints and perform a simple type input validation when handling the program arguments.</p><h2 id=train-a-model-and-generate-some-names>Train a model and generate some names<a href=#train-a-model-and-generate-some-names class=anchor aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 015 5 5 5 0 01-5 5h-3m-6 0H6a5 5 0 01-5-5 5 5 0 015-5h3"/><line x1="8" y1="12" x2="16" y2="12"/></svg></a></h2><p>Its time to install the project dependencies, create some working directory for temporary files, train a model and generate some names.</p><p>Inside the project directory and your virtual environment run something like the following</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>poetry install
</span></span><span class=line><span class=cl>mkdir WD
</span></span><span class=line><span class=cl>onomatico vocab data/names.csv WD/vocab.pt
</span></span><span class=line><span class=cl>onomatico train data WD/vocab.pt <span class=m>50</span> WD/model.pt --disable-wandb
</span></span><span class=line><span class=cl>onomatico generate WD/model.pt WD/vocab.pt <span class=m>900</span> WD/new_names.csv
</span></span><span class=line><span class=cl>cat WD/new_names.csv
</span></span></code></pre></div><p>Congratulations you have created a set of 900 new names. Lets have a peek at
them quickly.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>names
</span></span><span class=line><span class=cl>Joshan MARTIS
</span></span><span class=line><span class=cl>Christie TARE
</span></span><span class=line><span class=cl>Zachar KEKS
</span></span><span class=line><span class=cl>Chttew GOJEZ
</span></span><span class=line><span class=cl>Limon DARTISON
</span></span><span class=line><span class=cl>Nichathelld GUEZ
</span></span><span class=line><span class=cl>Chelles RARD
</span></span><span class=line><span class=cl>Edonda GERTIZSON
</span></span><span class=line><span class=cl>Chrim SCOTTEZ
</span></span><span class=line><span class=cl>Aaridony MOWNEZ
</span></span><span class=line><span class=cl>Chonarl TUGUE
</span></span><span class=line><span class=cl>Michele MUNERES
</span></span><span class=line><span class=cl>Ronald JAYAN
</span></span><span class=line><span class=cl>Jotthhan PERTERE
</span></span><span class=line><span class=cl>...
</span></span></code></pre></div><p>Not to bad, but one can spot already some issues with the names, but this
will bring is to the topic of data quality that will be covered in the
next blog post.</p><p>So stay tuned for more!</p><h2 id=conclusion>Conclusion<a href=#conclusion class=anchor aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 015 5 5 5 0 01-5 5h-3m-6 0H6a5 5 0 01-5-5 5 5 0 015-5h3"/><line x1="8" y1="12" x2="16" y2="12"/></svg></a></h2><p>Congratulations for making it through the tutorial. You hopefully got an basic understanding on how to build a generative model and how to use the tutorial project
to generate new names.</p><p>But what do you think about those generated names? How do they compare with the
actual names (i.e training data)?
Are these simple copies or shuffles of the original names? Do the names make any
sense, or are they only random sequence of characters?
How much similarity and how much novelty do we want in our new names, and how do
we quantify the result?</p><p>These and other questions we are going to discuss in the next blog post of this
series.</p></div></main><footer id=site-footer class="section-inner thin animated fadeIn faster"><p>&copy; 2023 <a href=https://www.manuelpasieka.com>Manuel Pasieka</a></p></footer><script src=https://www.manuelpasieka.com/js/bundle.min.c862700a0784e61f9b5c3f2adb6d843ecb95842ad8bb6f5e97a2a18f58bf4d20.js integrity="sha256-yGJwCgeE5h+bXD8q222EPsuVhCrYu29el6Khj1i/TSA=" crossorigin=anonymous></script></body></html>